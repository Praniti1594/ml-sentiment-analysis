ğŸ”µ 1. C (Regularization Strength)
ğŸ‘‰ What it means

C controls how strictly the model avoids overfitting.

Low C (0.1, 0.01)
Strong regularization â†’ model becomes simple
Might underfit.

High C (1, 10)
Weak regularization â†’ model becomes more flexible
Can overfit.

ğŸ‘‰ Real-world analogy

C = â€œallow the model to memorize vs generalizeâ€.

Small C â†’ â€œDonâ€™t memorize too muchâ€

Large C â†’ â€œYou can memorize more patterns if you wantâ€

ğŸ‘‰ Effect:

You changed C â†’ accuracy changed â†’ you found the best value.

â­ What does penalty mean in Logistic Regression?

The penalty tells the model:

ğŸ‘‰ â€œHow strict should I be while learning?
Should I keep weights small or remove some?â€

Weights = the importance the model gives to each word.
There are two main penalties:

ğŸ”µ L2 Penalty (most common, and you used this)
Simple meaning:

â¡ï¸ Makes the model keep all words but reduces their importance slightly.

Why?
So the model doesnâ€™t overfit or become too confident.

Think of it like:
â€œYou can use every word, but donâ€™t rely too heavily on any one word.â€
Good for:
âœ” TF-IDF
âœ” Sentiment analysis
âœ” Big text datasets

ğŸ”µ L1 Penalty
Simple meaning:
â¡ï¸ Makes the model delete many unimportant words (weights become zero).
So only a few important words stay.
Think of it like:
â€œChoose only the MOST important words. Ignore the rest.â€
Good for:
âœ” Feature selection
âœ” Smaller datasets
âŒ Not ideal for large TF-IDF text data

â­ In this project:

  we used L2 because:

âœ” It works best with TF-IDF
âœ” Gives higher accuracy in sentiment tasks
âœ” More stable
âœ” Learns smoothly without deleting features

â­ What is max_iter?

When a model learns, it keeps adjusting its weights again and again to get better.

ğŸ‘‰ Each adjustment = 1 iteration

So:
max_iter = How many chances you give the model to learn.

â­ Why increase it?

Because text data (sentences â†’ TF-IDF) creates thousands of features.

If you allow too few iterations:

âŒ The model stops learning halfway
âŒ Accuracy becomes low
âŒ It shows warnings like â€œFailed to convergeâ€

If you allow enough iterations:

âœ” The model fully learns
âœ” Accuracy improves
âœ” No warnings

â­ Super Simple Example

Imagine teaching a kid:

If you teach them 10 times â†’ maybe they still donâ€™t understand

If you teach them 300 times â†’ they fully understand the concept

Same logic.

â­ In our project:

we used something like:

LogisticRegression(max_iter=300)


âœ” This gave the model enough time to fully learn
âœ” Thatâ€™s why Logistic Regression accuracy improved

â­ Does max_iter = 1000 give more accuracy?
âœ… Only helps if:

Your model has not finished learning yet.

Meaning:
If at max_iter = 300 the model is still struggling to converge, increasing to 1000 MAY help.

âŒ But usually:

By the time it reaches 200â€“300 iterations, the model has already learned everything it can.

After that:

Accuracy does not improve

Loss barely changes

Extra iterations = wasted time

Model starts overfitting if pushed too much

â­ How to check?

When training Logistic Regression, did you see this warning?

ConvergenceWarning: lbfgs failed to converge. Increase max_iter.


If YES â†’ then increase to 500 or 1000.

If NO â†’ your model already converged.
Increasing to 1000 changes nothing.

ğŸ”µ 4. solver (Optimization Algorithm)

The solver decides how Logistic Regression updates its weights during training.
Different solvers work better for different types of data.

âœ” Common Solver Options
Solver : What it does. 	When to use it

liblinear: 	Uses coordinate descent. Supports L1 & L2.	Small datasets. Binary classification. Feature selection (L1).

lbfgs: 	Uses quasi-Newton optimization. Only L2 penalty.	Large datasets. Fast and stable. Best for most text tasks.

saga: Stochastic gradient with variance reduction. Supports L1 & L2.	Very large + sparse datasets (like TF-IDF). Best for high-dimensional text.

